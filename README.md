# Bootcamp de Engenharia de Dados - Nova Drive Motors

Este reposit√≥rio cont√©m o desenvolvimento do projeto de bootcamp para a **Nova Drive Motors**, focado na constru√ß√£o de um Data Warehouse (DW) para a √°rea de Vendas.

O objetivo √© criar um pipeline de dados completo, desde a extra√ß√£o de dados de um banco operacional at√© a disponibiliza√ß√£o em uma camada anal√≠tica para os gestores.

## üöÄ Arquitetura Proposta

O fluxo de dados seguir√° a seguinte arquitetura:

**Postgres (BD Operacional) ‚Üí Airflow (Orquestra√ß√£o) ‚Üí Snowflake (BD Stage) ‚Üí dbt (Transforma√ß√£o) ‚Üí Snowflake (BD Anal√≠tico)**

---

## üìÇ Estrutura do Projeto

-   `.devcontainer/`: Configura√ß√µes do ambiente de desenvolvimento Docker.
-   `airflow/`: Arquivos do Apache Airflow (DAGs, plugins, logs).
-   `dbt/`: Projeto dbt para a camada de transforma√ß√£o.
-   `scripts/`: Scripts de apoio (como a an√°lise explorat√≥ria).
-   `modeling.md`: Documenta√ß√£o detalhada do modelo dimensional.

---

## üõ†Ô∏è Fase 1: Ambiente de Desenvolvimento

Esta fase estabelece a base para todo o desenvolvimento, garantindo um ambiente de trabalho consistente e validando o acesso √† fonte de dados.

### Como Iniciar o Ambiente:
1.  Instale o [Docker Desktop](https://www.docker.com/products/docker-desktop/ ) e o [Visual Studio Code](https://code.visualstudio.com/ ).
2.  Instale a extens√£o [Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers ) da Microsoft no VS Code.
3.  Clone este reposit√≥rio: `git clone https://github.com/luccagouveia/novadrive-data-engineering-bootcamp.git`
4.  Abra a pasta do projeto no VS Code.
5.  Clique na notifica√ß√£o **"Reopen in Container"** que aparecer√° no canto inferior direito.

---

## üìä Fase 2: Explora√ß√£o e Modelagem de Dados

Nesta fase, analisamos o banco de dados operacional para entender a estrutura dos dados e definimos o modelo do nosso Data Warehouse.

### 1. An√°lise Explorat√≥ria
-   Um script Python (`scripts/explore_database_schema.py` ) foi criado para se conectar ao banco de dados e extrair metadados das tabelas principais (`clientes`, `vendas`, etc.).
-   O script analisa a contagem de linhas, a estrutura das colunas e uma amostra dos dados.

### 2. Modelo Dimensional
-   Foi definido um **Esquema Estrela (Star Schema)** para o DW.
-   **Tabela Fato:** `Fato_Vendas`
-   **Tabelas de Dimens√£o:** `Dim_Cliente`, `Dim_Veiculo`, `Dim_Concessionaria`, `Dim_Data`.
-   A documenta√ß√£o completa do modelo est√° no arquivo **[modeling.md](./modeling.md)**.

---

## üß™ Acessos para Teste e Valida√ß√£o

Para simular o ambiente real e validar o pipeline, os seguintes acessos podem ser utilizados:

### 1. Banco de Dados Operacional (Leitura)
-   **Host:** ``
-   **Porta:** ``
-   **Banco de Dados:** ``
-   **Usu√°rio:** ``
-   **Senha:** ``

### 2. Sistema de Vendas (Inser√ß√£o de Dados)
√â poss√≠vel inserir novos dados de vendas atrav√©s da interface web, que se refletir√£o no banco de dados acima.
-   **URL:** ``
-   **Login:** ``
-   **Senha:** ``
-   **Consulta por ID:** ``

---

## ‚è© Pr√≥ximas Etapas do Projeto

-   [ ] **Etapa 3:** Configurar o Apache Airflow.
-   [ ] **Etapa 4:** Configurar o Snowflake.
-   [ ] **Etapa 5:** Criar a camada Stage no Snowflake via Airflow.
-   [ ] **Etapa 6:** Criar e configurar a camada anal√≠tica com dbt.
